Et ‚ÄúWeb API‚Äù (nt FastAPI backend) Streamlit‚Äôi chatboti ette panna, teed nii:

Streamlit UI kogub s√µnumid ja hoiab st.session_state.messages ajalugu

Iga uue prompt-iga teeb HTTP POST sinu backend‚Äôi endpointi (nt /chat)

Backend tagastab JSON-i (nt { "reply": "..." }) ja UI lisab selle ajalukku

Allpool on t√∂√∂tav minimaalne n√§ide.

1) Streamlit: lisa vestlusajalugu + API-k√µne
import streamlit as st
import requests

st.set_page_config(page_title="Minu Esimene √Ñpp", page_icon="ü§ñ")

st.title("üéì AI Kursuse N√µustaja")
st.caption("Lihtne vestlusliides Web API kaudu.")

API_URL = "http://127.0.0.1:8000/chat"  # FastAPI endpoint

# 1) Algatame vestluse ajaloo, kui seda veel pole
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Tere! Kirjelda, mida soovid √µppida."}
    ]

# 2) Kuvame vestluse senise ajaloo
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

def call_chat_api(messages):
    """
    Saadame kogu ajaloo (v√µi viimased N s√µnumit) backendile.
    Backend otsustab, kuidas seda LLM-ile anda.
    """
    payload = {"messages": messages}
    r = requests.post(API_URL, json=payload, timeout=60)
    r.raise_for_status()
    return r.json()["reply"]

# 3) Korjame √ºles uue kasutaja sisendi
if prompt := st.chat_input("Kirjelda, mida soovid √µppida..."):
    # Kuvame kohe kasutaja s√µnumi ja salvestame selle ajalukku
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Kuvame vastuse ja salvestame ajalukku
    with st.chat_message("assistant"):
        with st.spinner("M√µtlen..."):
            try:
                reply = call_chat_api(st.session_state.messages)
            except requests.RequestException as e:
                reply = f"API viga: {e}"
            st.markdown(reply)

    st.session_state.messages.append({"role": "assistant", "content": reply})


Kui tahad saata backendile ainult viimased N s√µnumit, tee:
call_chat_api(st.session_state.messages[-10:])

2) FastAPI: tee /chat endpoint (mock v√µi LLM-iga)
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Literal

app = FastAPI()

Role = Literal["user", "assistant", "system"]

class Msg(BaseModel):
    role: Role
    content: str

class ChatRequest(BaseModel):
    messages: List[Msg]

class ChatResponse(BaseModel):
    reply: str

@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    # MINIMAALNE: automaatvastus viimasele user s√µnumile
    last_user = next((m.content for m in reversed(req.messages) if m.role == "user"), "")
    return ChatResponse(reply=f"Sain su s√µnumi: '{last_user}'. (LLM siia hiljem)")


K√§ivitamine:

uvicorn main:app --reload --port 8000

3) Kuidas see ‚Äú√µige‚Äù chatboti arhitektuurina v√§lja n√§eb

Streamlit: ainult UI + session_state (kerge, ei hoia salajasi v√µtmeid)

Web API (FastAPI):

hoiab LLM API v√µtmeid / Ollama hosti seadeid

teeb RAG otsingu (vektorbaas)

koostab prompti + kutsub LLM-i

tagastab reply

4) Kui tahad ‚Äústreaming‚Äù vastust (t√ºpib jooksvalt)

Siis muutub nii:

FastAPI peab andma SSE / chunked vastuseid

Streamlit pool peab lugema t√ºkke ja uuendama st.empty()-ga
Kui see on su j√§rgmine samm, √ºtlen t√§pselt milline SSE n√§ide valida (FastAPI StreamingResponse + Streamlit requests streaming).